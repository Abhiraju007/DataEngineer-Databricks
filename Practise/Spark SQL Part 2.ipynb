{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2da07e-7168-44b8-b8e4-8f4b474411e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n|Seqno|               Quote| id|\n+-----+--------------------+---+\n|    1|Be the change tha...|  1|\n|    2|Everyone thinks o...|  2|\n|    3|The purpose of ou...|  3|\n|    4|            Be cool.|  4|\n+-----+--------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "columns = [\"Seqno\",\"Quote\",\"id\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\",1),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\",2),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\",3),\n",
    "    (\"4\", \"Be cool.\",4)]\n",
    "df1 = spark.createDataFrame(data,columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f64d4f-805f-4c33-9d40-907eca4706ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+---+\n|Seqno|Quote                                                                        |id |\n+-----+-----------------------------------------------------------------------------+---+\n|1    |Be the change that you wish to see in the world                              |1  |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|2  |\n|3    |The purpose of our lives is to be happy.                                     |3  |\n|4    |Be cool.                                                                     |4  |\n+-----+-----------------------------------------------------------------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87cbc2a-eb5c-4fa5-88b2-b1cae2d8234f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cid</th><th>cname</th></tr></thead><tbody><tr><td>1</td><td>rama</td></tr><tr><td>2</td><td>null</td></tr><tr><td>3</td><td>sai</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "rama"
        ],
        [
         2,
         null
        ],
        [
         3,
         "sai"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cid",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cname",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = [(1,'rama'),(2,None),(3,'sai')]\n",
    "headers = [\"cid\",\"cname\"]\n",
    "df = spark.createDataFrame(l,headers)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c7fd68-15ab-468b-855e-855e6857c732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  2| null|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_null = df.where('cname is null')\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132e3cde-d7a1-4dea-b336-97506bf0c619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  1| rama|\n|  3|  sai|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_null = df.where('cname is not null')\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634e9131-0f2c-4f05-86d2-8aeae35b8c07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  2| null|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_null = df.where(col('cname').isNull())\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1af9869-9450-493e-8e2f-ec1b4d103ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  1| rama|\n|  3|  sai|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_null = df.where(col('cname').isNotNull())\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8d20e4-1aff-41ae-baba-0ffa50006462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType='sse-s3'),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType='sse-s3'),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType='sse-s3'),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='sse-s3')]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b1a0d35-dd55-48aa-a759-b24cb6b105cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6e22c2-5214-40b3-bb47-ed5a5d37b6b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33437396-cc83-4981-b14f-47d84ab2c8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n|        2|   Master|       Rama|         770|\n|        2|     Visa|       Rama|        1240|\n|        3|   Master|    Krishna|        1140|\n|        3|     Visa|    Krishna|        1140|\n|        6|   Master|      Srinu|        1240|\n|        7|     Visa|      Srinu|        1240|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.csv('dbfs:/FileStore/Credit_Pyspark.csv',header=True)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67c7d47-29d1-440a-995d-70e151080daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n|        2|   Master|       Rama|         770|\n|        2|     Visa|       Rama|        1240|\n|        3|   Master|    Krishna|        1140|\n|        3|     Visa|    Krishna|        1140|\n|        6|   Master|      Srinu|        1240|\n|        7|     Visa|      Srinu|        1240|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "file_path = 'dbfs:/FileStore/Credit_Pyspark.csv'\n",
    "file_format = 'csv'\n",
    "first_row_header = True\n",
    "inferSchema = True\n",
    "df_csv = spark.read.format(file_format).option(\"header\",first_row_header).option(\"inferSchema\",inferSchema).load(file_path)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fc7d99-a9bb-4a2b-992b-8c7546a57b65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `print(l)` not found.\n+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        2|   Master|       Rama|         770|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#To get first index value\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "l = df_csv.agg({\"Credit_Score\":\"min\"}).first()[0]\n",
    "#print(l)\n",
    "#To print entire row\n",
    "df_result = df_csv.filter(col('Credit_Score')==l)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a054ac3-d5d4-4432-8599-29757860ac21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+\n|Card_Type|collect_list(Credit_Score)|\n+---------+--------------------------+\n|Visa     |[978, 1240, 1140, 1240]   |\n|Master   |[978, 770, 1140, 1240]    |\n+---------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by\n",
    "\n",
    "#grouping on Card_type with credit Score\n",
    "df_csv.groupBy(\"Card_Type\").agg({\"Credit_Score\":\"collect_list\"}).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435aff56-54fe-4058-9235-2dbe46fade76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+\n|Card_Type|collect_set(Credit_Score)|\n+---------+-------------------------+\n|Visa     |[978, 1240, 1140]        |\n|Master   |[978, 1240, 1140, 770]   |\n+---------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Collect Set Remove the duplicates while grouping\n",
    "df_csv.groupBy(\"Card_Type\").agg({\"Credit_Score\":\"collect_set\"}).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d4e687-c3c2-4f8a-a249-e36f82e62e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n|Card_Type|Creditscore            |\n+---------+-----------------------+\n|Visa     |[978, 1240, 1140, 1240]|\n|Master   |[978, 770, 1140, 1240] |\n+---------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.collect_list(\"Credit_Score\").alias(\"Creditscore\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff0bec3-8b27-429b-a535-2c1a7bd9a22e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        2|     Visa|       Rama|        1240|\n|        6|   Master|      Srinu|        1240|\n|        7|     Visa|      Srinu|        1240|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# to get maximum value\n",
    "\n",
    "first_row = df_csv.agg({\"Credit_Score\":\"max\"}).first()[0]\n",
    "\n",
    "#to get all record of maximum credit score\n",
    "df_csv.filter(col(\"Credit_Score\")==first_row).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e80f151-d77e-4977-988a-d5bcbef9f886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+\n|Card_Type|collect_set(Credit_Score)|\n+---------+-------------------------+\n|Visa     |[978, 1240, 1140]        |\n|Master   |[978, 1240, 1140, 770]   |\n+---------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.collect_set(\"Credit_Score\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b96e18c-6615-4843-a3b0-f313f4368ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n|Card_Type|approx_count_distinct(credit_score)|\n+---------+-----------------------------------+\n|     Visa|                                  3|\n|   Master|                                  4|\n+---------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Count approximate Distinct number\n",
    "df_csv.groupBy(\"Card_Type\").agg({\"credit_score\":\"approx_count_distinct\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389378bf-9f54-4932-bbd5-d74803da8a9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|Card_Type|Creditscore|\n+---------+-----------+\n|Visa     |3          |\n|Master   |4          |\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.approx_count_distinct(\"Credit_Score\").alias(\"Creditscore\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e72c40-98b0-41a2-aafc-57ec5360756d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n|Card_Type|sum(Credit_Score)|\n+---------+-----------------+\n|     Visa|             4598|\n|   Master|             4128|\n+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#sum of all credit score\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg({\"Credit_Score\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb9a61e-bd69-48f2-b173-faa582a6832d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n|Card_Type|Total_Credit_Score|\n+---------+------------------+\n|Visa     |4598              |\n|Master   |4128              |\n+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.sum(\"Credit_Score\").alias(\"Total_Credit_Score\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcb8aa6-a743-45c9-8ebc-0b0a00f62315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n|Card_Type|Distinct_Total_Credit_Score|\n+---------+---------------------------+\n|Visa     |3358                       |\n|Master   |4128                       |\n+---------+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_csv.groupBy('Card_Type').agg(F.sum_distinct(\"Credit_Score\").alias(\"Distinct_Total_Credit_Score\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad71e1db-1bfc-4b66-a6b8-de77770ea579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n|Card_Type|avg(Credit_Score)|\n+---------+-----------------+\n|     Visa|           1149.5|\n|   Master|           1032.0|\n+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.groupBy(\"Card_Type\").agg({\"Credit_Score\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bd235b-59d4-46e1-9dda-7d8530f3f14a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n|Card_Type|Average_Credit_Score|\n+---------+--------------------+\n|Visa     |1149.5              |\n|Master   |1032.0              |\n+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.avg(\"Credit_Score\").alias(\"Average_Credit_Score\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0f7ce9-cdd0-408f-8a41-52edaad49c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n|Card_Type|count(Credit_Score)|\n+---------+-------------------+\n|     Visa|                  4|\n|   Master|                  4|\n+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# count the card type on credit Score\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg({\"Credit_Score\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3c71d9-4490-46a6-8f5d-49e6a986704b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n|Card_Type|Count of Credit Score|\n+---------+---------------------+\n|Visa     |4                    |\n|Master   |4                    |\n+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_csv.groupBy(\"Card_Type\").agg(F.count(\"Credit_Score\").alias(\"Count of Credit Score\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77d472c-5056-4d61-a69a-e831f47ed900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.filter(\"Credit_Name like 'A%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ab0970-3402-4b6a-91ee-2649d1731b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n|        2|   Master|       Rama|         770|\n|        2|     Visa|       Rama|        1240|\n|        3|   Master|    Krishna|        1140|\n|        3|     Visa|    Krishna|        1140|\n|        6|   Master|      Srinu|        1240|\n|        7|     Visa|      Srinu|        1240|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5e2073-5a13-4375-9bf9-0ebb6efa9565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_csv.filter(col(\"Credit_Name\").like('A%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b44d3ed-3ad8-4c00-abfa-32def601b176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n|        2|   Master|       Rama|         770|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Between operator\n",
    "\n",
    "df_csv.filter(\"Credit_Score between 700 and 1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd71070b-cf8b-4b4e-8910-f371a71c7416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+------------+\n|Credit_ID|Card_Type|Credit_Name|Credit_Score|\n+---------+---------+-----------+------------+\n|        2|   Master|       Rama|         770|\n|        1|   Master|       Abhi|         978|\n|        1|     Visa|       Abhi|         978|\n+---------+---------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_csv.filter(col(\"Credit_Score\").between(700,1000)).orderBy(\"Credit_Score\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77da7e20-435a-4fd5-8a8f-0a20ef7e67e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([('rama','india'),('sai','india'),('rama','india'),('sai','india')],['cname','ccountry'])\n",
    "df2 = spark.createDataFrame([('david','usa'),('roshan','usa')],['cname','ccountry'])\n",
    "df3 = spark.createDataFrame([('john','uk'),('miller','uk')],['cname','ccountry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33be7120-3cbe-4c41-b19c-93fd257375f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n|cname|ccountry|\n+-----+--------+\n| rama|   india|\n|  sai|   india|\n| rama|   india|\n|  sai|   india|\n+-----+--------+\n\nNone\n+------+--------+\n| cname|ccountry|\n+------+--------+\n| david|     usa|\n|roshan|     usa|\n+------+--------+\n\nNone\n+------+--------+\n| cname|ccountry|\n+------+--------+\n|  john|      uk|\n|miller|      uk|\n+------+--------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "print(df1.show())\n",
    "print(df2.show())\n",
    "print(df3.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abd6122-dcec-479b-9aa5-d3f3291a577c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n| cname|ccountry|\n+------+--------+\n|  rama|   india|\n|   sai|   india|\n|  rama|   india|\n|   sai|   india|\n| david|     usa|\n|roshan|     usa|\n|  john|      uk|\n|miller|      uk|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#union operation\n",
    "\n",
    "final_df =df1.union(df2).union(df3)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17479015-652d-464c-b8dc-133fb5144f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n| cname|ccountry|\n+------+--------+\n|  rama|   india|\n|   sai|   india|\n|  rama|   india|\n|   sai|   india|\n| david|     usa|\n|roshan|     usa|\n|  john|      uk|\n|miller|      uk|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#reduce the operation, to bypass the again union operation we use this\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "df_lst = [df1,df2,df3]\n",
    "\n",
    "final_df1 = reduce(DataFrame.unionAll,df_lst)\n",
    "final_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec4dab1-c300-456f-99c9-406da70b85c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n| cname|ccountry|\n+------+--------+\n|  rama|   india|\n|   sai|   india|\n|  rama|   india|\n|   sai|   india|\n| david|     usa|\n|roshan|     usa|\n|  john|      uk|\n|miller|      uk|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#reduce the operation, to bypass the again union operation we use this\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "df_lst = [df1,df2,df3]\n",
    "\n",
    "final_df1 = reduce(DataFrame.union,df_lst)\n",
    "final_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41bc4af-b800-45c6-925a-124f457baf83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  2| null|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_null = df.filter(col('cname').isNull())\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001f22d4-6a54-4781-aa88-8a3a89ac0003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|cid|cname|\n+---+-----+\n|  1| rama|\n|  2| null|\n|  3|  sai|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c901b568-e655-4210-8ff2-c871b7a7d431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark SQL Part 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
